{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5005082c18761978",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d635ad5-472d-4212-9f66-13674b7fd3a3",
   "metadata": {},
   "source": [
    "<h3>Final submission notebook with code</h3>\n",
    "\n",
    "<b>Team: </b>\n",
    "<ul>\n",
    "    <li>Mironov Mikhail - mvmironov@edu.hse.ru</li>\n",
    "</ul>\n",
    "\n",
    "<p>We have a dedicated Github repository for this project where we have a plethora of other notebooks stored in different branches, there are too many merge conflicts now to pull into a single presentable master branch. This notebook is an aggregation of the best things that we have achieved during the time of the project.</p>\n",
    "\n",
    "<b>Rundown on the aim of the project</b>\n",
    "\n",
    "<h4>Data Description</h4>\n",
    "\n",
    "<p>We have collected data from Binance API. We are using tick level data which is the data containing information about all trades executed for given currency pairs like BTC/USDT and many others. In fact we focused our analysis on currency pairs that are traded against USDT. On our github you can find the code and instructions to run the code to collect tick level data from Binance. In our case it took roughly 20 minutes to download all trades for all currency pairs traded against USDT for November 2024.</p>\n",
    "\n",
    "<h3>Data pipeline</h3>\n",
    "\n",
    "<h4>Pipeline to load tick data</h4>\n",
    "We are downloading compressed zip files from Binance Datavision website which contains aggregated tick data in either daily or monthly chunks. Then after collecting all of the data into its separate folder, we are running the transforming pipeline that unpacks all of the zipped csv files, reads them and discards of the unnecessary fields. Since the compressed csv files are quite big, they could reach the size of 5GB, it is quite hard to fit into the memory in one go and do something with it especially wrap into pd.DataFrame. To solve this issue, we used Polars library that can scan files and do computations in lazy manner, we also applied batching to read csv files in batches of 128 MBs and then dump the data in parquet files with the Hive Dataset structure using Pyarrow. As a result, we have the following folder structure with data on our working machine:\n",
    "\n",
    "On the first level we have dates, on the second we have tickers, this way we can scan necessary data efficiently and load the data that we need. \n",
    "\n",
    "<h4>Feature computation pipeline</h4>\n",
    "\n",
    "<p>We took cross-sections with all assets traded at the time of each cross-section. Then, we took each cross-section and computed features for each currency_pair within cross-section. We had to use multiprocessing along with Polars to allow us to process more cross-sections as loading and doing calculations with tick level data is quite costly. As a result, we obtained a dataset with the structure below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3f27b9-3fa5-4bf7-bea9-a5f58abd3cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T03:43:54.229399Z",
     "start_time": "2024-12-22T03:43:54.037499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency_pair</th>\n",
       "      <th>log_return_MINUTE</th>\n",
       "      <th>log_return_FIVE_MINUTES</th>\n",
       "      <th>log_return_FIFTEEN_MINUTES</th>\n",
       "      <th>log_return_HALF_HOUR</th>\n",
       "      <th>log_return_HOUR</th>\n",
       "      <th>log_return_TWO_HOURS</th>\n",
       "      <th>log_return_FOUR_HOURS</th>\n",
       "      <th>log_return_TWELVE_HOURS</th>\n",
       "      <th>log_return_DAY</th>\n",
       "      <th>...</th>\n",
       "      <th>mle_alpha_powerlaw_HOUR</th>\n",
       "      <th>mle_alpha_powerlaw_TWO_HOURS</th>\n",
       "      <th>mle_alpha_powerlaw_FOUR_HOURS</th>\n",
       "      <th>mle_alpha_powerlaw_TWELVE_HOURS</th>\n",
       "      <th>mle_alpha_powerlaw_DAY</th>\n",
       "      <th>mle_alpha_powerlaw_THREE_DAYS</th>\n",
       "      <th>mle_alpha_powerlaw_WEEK</th>\n",
       "      <th>log_return</th>\n",
       "      <th>cross_section_start_time</th>\n",
       "      <th>cross_section_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STORJUSDT</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.004094</td>\n",
       "      <td>-0.034107</td>\n",
       "      <td>-0.034107</td>\n",
       "      <td>...</td>\n",
       "      <td>1.211706</td>\n",
       "      <td>1.214854</td>\n",
       "      <td>1.220459</td>\n",
       "      <td>1.190168</td>\n",
       "      <td>1.189988</td>\n",
       "      <td>1.182173</td>\n",
       "      <td>1.182826</td>\n",
       "      <td>-0.013677</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LITUSDT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.016375</td>\n",
       "      <td>-0.049707</td>\n",
       "      <td>-0.063618</td>\n",
       "      <td>...</td>\n",
       "      <td>1.643776</td>\n",
       "      <td>1.571583</td>\n",
       "      <td>1.236279</td>\n",
       "      <td>1.174790</td>\n",
       "      <td>1.161264</td>\n",
       "      <td>1.164203</td>\n",
       "      <td>1.158009</td>\n",
       "      <td>-0.010664</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUSHIUSDT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001278</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>-0.008866</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.027433</td>\n",
       "      <td>0.122408</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239298</td>\n",
       "      <td>1.168115</td>\n",
       "      <td>1.150065</td>\n",
       "      <td>1.146371</td>\n",
       "      <td>1.147874</td>\n",
       "      <td>1.149215</td>\n",
       "      <td>1.145589</td>\n",
       "      <td>-0.013427</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>-0.005295</td>\n",
       "      <td>-0.067101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.152814</td>\n",
       "      <td>1.152017</td>\n",
       "      <td>1.150975</td>\n",
       "      <td>1.147125</td>\n",
       "      <td>1.148583</td>\n",
       "      <td>1.150560</td>\n",
       "      <td>1.151495</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TIAUSDT</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.007680</td>\n",
       "      <td>-0.021877</td>\n",
       "      <td>-0.021877</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222930</td>\n",
       "      <td>1.164009</td>\n",
       "      <td>1.146473</td>\n",
       "      <td>1.123734</td>\n",
       "      <td>1.125200</td>\n",
       "      <td>1.126143</td>\n",
       "      <td>1.125708</td>\n",
       "      <td>-0.028495</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  currency_pair  log_return_MINUTE  log_return_FIVE_MINUTES  \\\n",
       "0     STORJUSDT          -0.000195                -0.000195   \n",
       "1       LITUSDT           0.000000                 0.001184   \n",
       "2     SUSHIUSDT           0.000000                -0.001278   \n",
       "3       ETHUSDT          -0.000145                 0.000121   \n",
       "4       TIAUSDT          -0.000205                -0.000205   \n",
       "\n",
       "   log_return_FIFTEEN_MINUTES  log_return_HALF_HOUR  log_return_HOUR  \\\n",
       "0                    0.004486             -0.000389        -0.000389   \n",
       "1                    0.000000              0.000000         0.000000   \n",
       "2                    0.001913             -0.006349        -0.006349   \n",
       "3                   -0.000678              0.002248        -0.001256   \n",
       "4                    0.000000              0.000000         0.007945   \n",
       "\n",
       "   log_return_TWO_HOURS  log_return_FOUR_HOURS  log_return_TWELVE_HOURS  \\\n",
       "0             -0.000389              -0.004094                -0.034107   \n",
       "1              0.004706               0.016375                -0.049707   \n",
       "2             -0.008866               0.001895                 0.027433   \n",
       "3              0.006415               0.006686                -0.005295   \n",
       "4             -0.000406              -0.007680                -0.021877   \n",
       "\n",
       "   log_return_DAY  ...  mle_alpha_powerlaw_HOUR  mle_alpha_powerlaw_TWO_HOURS  \\\n",
       "0       -0.034107  ...                 1.211706                      1.214854   \n",
       "1       -0.063618  ...                 1.643776                      1.571583   \n",
       "2        0.122408  ...                 1.239298                      1.168115   \n",
       "3       -0.067101  ...                 1.152814                      1.152017   \n",
       "4       -0.021877  ...                 1.222930                      1.164009   \n",
       "\n",
       "   mle_alpha_powerlaw_FOUR_HOURS  mle_alpha_powerlaw_TWELVE_HOURS  \\\n",
       "0                       1.220459                         1.190168   \n",
       "1                       1.236279                         1.174790   \n",
       "2                       1.150065                         1.146371   \n",
       "3                       1.150975                         1.147125   \n",
       "4                       1.146473                         1.123734   \n",
       "\n",
       "   mle_alpha_powerlaw_DAY  mle_alpha_powerlaw_THREE_DAYS  \\\n",
       "0                1.189988                       1.182173   \n",
       "1                1.161264                       1.164203   \n",
       "2                1.147874                       1.149215   \n",
       "3                1.148583                       1.150560   \n",
       "4                1.125200                       1.126143   \n",
       "\n",
       "   mle_alpha_powerlaw_WEEK  log_return  cross_section_start_time  \\\n",
       "0                 1.182826   -0.013677                2025-01-01   \n",
       "1                 1.158009   -0.010664                2025-01-01   \n",
       "2                 1.145589   -0.013427                2025-01-01   \n",
       "3                 1.151495   -0.006211                2025-01-01   \n",
       "4                 1.125708   -0.028495                2025-01-01   \n",
       "\n",
       "   cross_section_end_time  \n",
       "0              2025-01-08  \n",
       "1              2025-01-08  \n",
       "2              2025-01-08  \n",
       "3              2025-01-08  \n",
       "4              2025-01-08  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df: pd.DataFrame = pd.read_parquet(r\"D:\\data\\features\\features_2025-05-11.parquet\")\n",
    "\n",
    "df[\"cross_section_start_time\"] = pd.to_datetime(df[\"cross_section_start_time\"])\n",
    "df[\"cross_section_end_time\"] = pd.to_datetime(df[\"cross_section_end_time\"])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3753f47d-22b2-4a01-a311-bd9a4c55fb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_return_MINUTE</th>\n",
       "      <td>387306.0</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.211844</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.060838</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_FIVE_MINUTES</th>\n",
       "      <td>420374.0</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.149743</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.212993</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_FIFTEEN_MINUTES</th>\n",
       "      <td>422160.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.291269</td>\n",
       "      <td>-0.00126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.188052</td>\n",
       "      <td>0.004138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_HALF_HOUR</th>\n",
       "      <td>422394.0</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.291269</td>\n",
       "      <td>-0.001787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.215458</td>\n",
       "      <td>0.005747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_HOUR</th>\n",
       "      <td>422495.0</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>-0.387939</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.34046</td>\n",
       "      <td>0.008247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_TWO_HOURS</th>\n",
       "      <td>422623.0</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.670737</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.767169</td>\n",
       "      <td>0.011651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_FOUR_HOURS</th>\n",
       "      <td>422641.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-1.091786</td>\n",
       "      <td>-0.005738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00578</td>\n",
       "      <td>0.767169</td>\n",
       "      <td>0.016864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_TWELVE_HOURS</th>\n",
       "      <td>422950.0</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-1.091786</td>\n",
       "      <td>-0.010159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.872299</td>\n",
       "      <td>0.028802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_DAY</th>\n",
       "      <td>423403.0</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>-1.692851</td>\n",
       "      <td>-0.0147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>1.669157</td>\n",
       "      <td>0.040483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_THREE_DAYS</th>\n",
       "      <td>425222.0</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-1.780307</td>\n",
       "      <td>-0.026242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025752</td>\n",
       "      <td>2.364366</td>\n",
       "      <td>0.06791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_WEEK</th>\n",
       "      <td>428454.0</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-2.554795</td>\n",
       "      <td>-0.042195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>2.555582</td>\n",
       "      <td>0.101908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_MINUTE</th>\n",
       "      <td>387306.0</td>\n",
       "      <td>0.51702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_FIVE_MINUTES</th>\n",
       "      <td>420374.0</td>\n",
       "      <td>0.512671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.509969</td>\n",
       "      <td>0.631799</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_FIFTEEN_MINUTES</th>\n",
       "      <td>422160.0</td>\n",
       "      <td>0.514424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43092</td>\n",
       "      <td>0.514496</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_HALF_HOUR</th>\n",
       "      <td>422394.0</td>\n",
       "      <td>0.512681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.443966</td>\n",
       "      <td>0.512671</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.121582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_HOUR</th>\n",
       "      <td>422495.0</td>\n",
       "      <td>0.512469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.512432</td>\n",
       "      <td>0.56993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_TWO_HOURS</th>\n",
       "      <td>422623.0</td>\n",
       "      <td>0.512147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463688</td>\n",
       "      <td>0.511885</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.091899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_FOUR_HOURS</th>\n",
       "      <td>422641.0</td>\n",
       "      <td>0.511803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.51135</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_TWELVE_HOURS</th>\n",
       "      <td>422950.0</td>\n",
       "      <td>0.51125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479135</td>\n",
       "      <td>0.51068</td>\n",
       "      <td>0.543247</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.065416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_DAY</th>\n",
       "      <td>423403.0</td>\n",
       "      <td>0.510855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484028</td>\n",
       "      <td>0.510318</td>\n",
       "      <td>0.537508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         count      mean       min       25%  \\\n",
       "log_return_MINUTE                     387306.0 -0.000002 -0.211844 -0.000268   \n",
       "log_return_FIVE_MINUTES               420374.0 -0.000002 -0.149743 -0.000704   \n",
       "log_return_FIFTEEN_MINUTES            422160.0  0.000002 -0.291269  -0.00126   \n",
       "log_return_HALF_HOUR                  422394.0 -0.000009 -0.291269 -0.001787   \n",
       "log_return_HOUR                       422495.0  -0.00001 -0.387939 -0.002684   \n",
       "log_return_TWO_HOURS                  422623.0  0.000037 -0.670737 -0.003859   \n",
       "log_return_FOUR_HOURS                 422641.0   0.00002 -1.091786 -0.005738   \n",
       "log_return_TWELVE_HOURS               422950.0 -0.000051 -1.091786 -0.010159   \n",
       "log_return_DAY                        423403.0 -0.000205 -1.692851   -0.0147   \n",
       "log_return_THREE_DAYS                 425222.0 -0.000525 -1.780307 -0.026242   \n",
       "log_return_WEEK                       428454.0 -0.001028 -2.554795 -0.042195   \n",
       "share_of_long_trades_MINUTE           387306.0   0.51702       0.0  0.333333   \n",
       "share_of_long_trades_FIVE_MINUTES     420374.0  0.512671       0.0       0.4   \n",
       "share_of_long_trades_FIFTEEN_MINUTES  422160.0  0.514424       0.0   0.43092   \n",
       "share_of_long_trades_HALF_HOUR        422394.0  0.512681       0.0  0.443966   \n",
       "share_of_long_trades_HOUR             422495.0  0.512469       0.0  0.455782   \n",
       "share_of_long_trades_TWO_HOURS        422623.0  0.512147       0.0  0.463688   \n",
       "share_of_long_trades_FOUR_HOURS       422641.0  0.511803       0.0  0.470588   \n",
       "share_of_long_trades_TWELVE_HOURS     422950.0   0.51125       0.0  0.479135   \n",
       "share_of_long_trades_DAY              423403.0  0.510855       0.0  0.484028   \n",
       "\n",
       "                                           50%       75%       max       std  \n",
       "log_return_MINUTE                          0.0  0.000268  0.060838  0.001214  \n",
       "log_return_FIVE_MINUTES                    0.0    0.0007  0.212993  0.002425  \n",
       "log_return_FIFTEEN_MINUTES                 0.0  0.001259  0.188052  0.004138  \n",
       "log_return_HALF_HOUR                       0.0  0.001784  0.215458  0.005747  \n",
       "log_return_HOUR                            0.0  0.002661   0.34046  0.008247  \n",
       "log_return_TWO_HOURS                       0.0  0.003891  0.767169  0.011651  \n",
       "log_return_FOUR_HOURS                      0.0   0.00578  0.767169  0.016864  \n",
       "log_return_TWELVE_HOURS                    0.0  0.010162  0.872299  0.028802  \n",
       "log_return_DAY                             0.0  0.014563  1.669157  0.040483  \n",
       "log_return_THREE_DAYS                      0.0  0.025752  2.364366   0.06791  \n",
       "log_return_WEEK                            0.0  0.041066  2.555582  0.101908  \n",
       "share_of_long_trades_MINUTE                0.5  0.730769       1.0  0.300607  \n",
       "share_of_long_trades_FIVE_MINUTES     0.509969  0.631799       1.0  0.193722  \n",
       "share_of_long_trades_FIFTEEN_MINUTES  0.514496       0.6       1.0  0.142611  \n",
       "share_of_long_trades_HALF_HOUR        0.512671  0.582278       1.0  0.121582  \n",
       "share_of_long_trades_HOUR             0.512432   0.56993       1.0  0.104058  \n",
       "share_of_long_trades_TWO_HOURS        0.511885  0.561224       1.0  0.091899  \n",
       "share_of_long_trades_FOUR_HOURS        0.51135  0.553527       1.0   0.08058  \n",
       "share_of_long_trades_TWELVE_HOURS      0.51068  0.543247       1.0  0.065416  \n",
       "share_of_long_trades_DAY              0.510318  0.537508       1.0  0.056783  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7cd42a-dfea-4c40-999f-b201e3d3b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"currency_pair\", \"cross_section_start_time\", \"cross_section_end_time\"])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4c9119-bf09-4ba0-bb74-e37deb4125c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_return is NaN, therefore there were no transcations during this period of time, hence the return is 0\n",
    "df[\"log_return\"] = df[\"log_return\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1192e0c-d70b-45d4-9477-0011815e83f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428827, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed67bc-0d95-4aaa-9442-9f69f9859eba",
   "metadata": {},
   "source": [
    "<h4>EDA. Check features distributions and search for possible bugs in the data pipeline</h4>\n",
    "\n",
    "<p>As we can see we have a lot of missing values, the good thing is that it is decreasing in TIME_OFFSET enum that we used to compute features on different intervals. This happens because for non-liquid orderbooks, there were no transactions within smaller intervals like FIVE_SECONDS, TEN_SECONDS and etc, but the good thing that it is decreasing as the interval grows to FIFTEEN_MINUTES which implies that this is just to the lack of transactions not the error in the pipeline</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a73de99-d3a7-4477-ba25-2677d5b47d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slippage_imbalance_MINUTE               168543\n",
       "slippage_imbalance_FIVE_MINUTES          54686\n",
       "log_return_MINUTE                        41521\n",
       "mle_alpha_powerlaw_MINUTE                41521\n",
       "share_of_long_trades_MINUTE              41521\n",
       "volume_imbalance_MINUTE                  41521\n",
       "slippage_imbalance_FIFTEEN_MINUTES       16456\n",
       "slippage_imbalance_HALF_HOUR              9503\n",
       "log_return_FIVE_MINUTES                   8453\n",
       "volume_imbalance_FIVE_MINUTES             8453\n",
       "share_of_long_trades_FIVE_MINUTES         8453\n",
       "mle_alpha_powerlaw_FIVE_MINUTES           8453\n",
       "slippage_imbalance_HOUR                   7527\n",
       "slippage_imbalance_TWO_HOURS              6831\n",
       "log_return_FIFTEEN_MINUTES                6667\n",
       "volume_imbalance_FIFTEEN_MINUTES          6667\n",
       "share_of_long_trades_FIFTEEN_MINUTES      6667\n",
       "mle_alpha_powerlaw_FIFTEEN_MINUTES        6667\n",
       "slippage_imbalance_FOUR_HOURS             6494\n",
       "log_return_HALF_HOUR                      6433\n",
       "mle_alpha_powerlaw_HALF_HOUR              6433\n",
       "volume_imbalance_HALF_HOUR                6433\n",
       "share_of_long_trades_HALF_HOUR            6433\n",
       "volume_imbalance_HOUR                     6332\n",
       "share_of_long_trades_HOUR                 6332\n",
       "mle_alpha_powerlaw_HOUR                   6332\n",
       "log_return_HOUR                           6332\n",
       "share_of_long_trades_TWO_HOURS            6204\n",
       "mle_alpha_powerlaw_TWO_HOURS              6204\n",
       "log_return_TWO_HOURS                      6204\n",
       "volume_imbalance_TWO_HOURS                6204\n",
       "mle_alpha_powerlaw_FOUR_HOURS             6186\n",
       "volume_imbalance_FOUR_HOURS               6186\n",
       "share_of_long_trades_FOUR_HOURS           6186\n",
       "log_return_FOUR_HOURS                     6186\n",
       "slippage_imbalance_TWELVE_HOURS           5951\n",
       "log_return_TWELVE_HOURS                   5877\n",
       "mle_alpha_powerlaw_TWELVE_HOURS           5877\n",
       "volume_imbalance_TWELVE_HOURS             5877\n",
       "share_of_long_trades_TWELVE_HOURS         5877\n",
       "slippage_imbalance_DAY                    5450\n",
       "share_of_long_trades_DAY                  5424\n",
       "mle_alpha_powerlaw_DAY                    5424\n",
       "volume_imbalance_DAY                      5424\n",
       "log_return_DAY                            5424\n",
       "slippage_imbalance_THREE_DAYS             3616\n",
       "share_of_long_trades_THREE_DAYS           3605\n",
       "volume_imbalance_THREE_DAYS               3605\n",
       "mle_alpha_powerlaw_THREE_DAYS             3605\n",
       "log_return_THREE_DAYS                     3605\n",
       "slippage_imbalance_WEEK                    384\n",
       "volume_imbalance_WEEK                      373\n",
       "log_return_WEEK                            373\n",
       "share_of_long_trades_WEEK                  373\n",
       "mle_alpha_powerlaw_WEEK                    373\n",
       "cross_section_start_time                     0\n",
       "currency_pair                                0\n",
       "log_return                                   0\n",
       "cross_section_end_time                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97cced44-882a-4c35-b9ff-4cf3fff66468",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cols: List[str] = list(\n",
    "    set(df.columns) - set([\"cross_section_start_time\", \"cross_section_end_time\", \"currency_pair\", \"log_return\"])\n",
    ")\n",
    "target_col: str = \"return\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129feddb-48dc-4a28-a153-0fa97d0758f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_return_FIVE_MINUTES',\n",
       " 'log_return_DAY',\n",
       " 'log_return_TWO_HOURS',\n",
       " 'log_return_FIFTEEN_MINUTES',\n",
       " 'log_return_HOUR',\n",
       " 'log_return_FOUR_HOURS',\n",
       " 'log_return_HALF_HOUR',\n",
       " 'log_return_THREE_DAYS',\n",
       " 'log_return_TWELVE_HOURS',\n",
       " 'log_return_WEEK',\n",
       " 'log_return_MINUTE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powerlaw_cols: List[str] = [col for col in reg_cols if \"mle\" in col]\n",
    "return_cols: List[str] = [col for col in reg_cols if col.startswith(\"log_return\")]\n",
    "return_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e9eb43-815c-42f7-8032-53bb6033922b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_return_TWO_HOURS</th>\n",
       "      <td>422623.0</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>-6.707373e-01</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.767169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_FIFTEEN_MINUTES</th>\n",
       "      <td>422160.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>-2.912688e-01</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.188052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume_imbalance_THREE_DAYS</th>\n",
       "      <td>425222.0</td>\n",
       "      <td>-0.016489</td>\n",
       "      <td>0.059414</td>\n",
       "      <td>-9.999861e-01</td>\n",
       "      <td>-0.040262</td>\n",
       "      <td>-0.015521</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume_imbalance_MINUTE</th>\n",
       "      <td>387306.0</td>\n",
       "      <td>0.020172</td>\n",
       "      <td>0.710638</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.654144</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.705307</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mle_alpha_powerlaw_FIVE_MINUTES</th>\n",
       "      <td>420374.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.007199e+15</td>\n",
       "      <td>1.320414</td>\n",
       "      <td>1.417477</td>\n",
       "      <td>1.580553</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippage_imbalance_THREE_DAYS</th>\n",
       "      <td>425211.0</td>\n",
       "      <td>-0.074882</td>\n",
       "      <td>0.248007</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.215651</td>\n",
       "      <td>-0.070563</td>\n",
       "      <td>0.063299</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_HOUR</th>\n",
       "      <td>422495.0</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>-3.879387e-01</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.340460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_FOUR_HOURS</th>\n",
       "      <td>422641.0</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.016864</td>\n",
       "      <td>-1.091786e+00</td>\n",
       "      <td>-0.005738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.767169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_FIVE_MINUTES</th>\n",
       "      <td>420374.0</td>\n",
       "      <td>0.512671</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.509969</td>\n",
       "      <td>0.631799</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippage_imbalance_DAY</th>\n",
       "      <td>423377.0</td>\n",
       "      <td>-0.086343</td>\n",
       "      <td>0.323507</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.290784</td>\n",
       "      <td>-0.083620</td>\n",
       "      <td>0.108531</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippage_imbalance_FOUR_HOURS</th>\n",
       "      <td>422333.0</td>\n",
       "      <td>-0.084263</td>\n",
       "      <td>0.481527</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.446715</td>\n",
       "      <td>-0.096129</td>\n",
       "      <td>0.251143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mle_alpha_powerlaw_FOUR_HOURS</th>\n",
       "      <td>422641.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.053494e+00</td>\n",
       "      <td>1.142608</td>\n",
       "      <td>1.189331</td>\n",
       "      <td>1.270284</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippage_imbalance_MINUTE</th>\n",
       "      <td>260284.0</td>\n",
       "      <td>0.019955</td>\n",
       "      <td>0.887137</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.063969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume_imbalance_HOUR</th>\n",
       "      <td>422495.0</td>\n",
       "      <td>-0.016561</td>\n",
       "      <td>0.229541</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.149253</td>\n",
       "      <td>-0.015223</td>\n",
       "      <td>0.115563</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippage_imbalance_HALF_HOUR</th>\n",
       "      <td>419324.0</td>\n",
       "      <td>-0.047319</td>\n",
       "      <td>0.685021</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.696307</td>\n",
       "      <td>-0.080916</td>\n",
       "      <td>0.582710</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_FOUR_HOURS</th>\n",
       "      <td>422641.0</td>\n",
       "      <td>0.511803</td>\n",
       "      <td>0.080580</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.511350</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_return_HALF_HOUR</th>\n",
       "      <td>422394.0</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>-2.912688e-01</td>\n",
       "      <td>-0.001787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.215458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_of_long_trades_MINUTE</th>\n",
       "      <td>387306.0</td>\n",
       "      <td>0.517020</td>\n",
       "      <td>0.300607</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume_imbalance_TWELVE_HOURS</th>\n",
       "      <td>422950.0</td>\n",
       "      <td>-0.018963</td>\n",
       "      <td>0.099134</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.066921</td>\n",
       "      <td>-0.016645</td>\n",
       "      <td>0.030042</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mle_alpha_powerlaw_THREE_DAYS</th>\n",
       "      <td>425222.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.053684e+00</td>\n",
       "      <td>1.117648</td>\n",
       "      <td>1.136728</td>\n",
       "      <td>1.163328</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      count      mean       std           min  \\\n",
       "log_return_TWO_HOURS               422623.0  0.000037  0.011651 -6.707373e-01   \n",
       "log_return_FIFTEEN_MINUTES         422160.0  0.000002  0.004138 -2.912688e-01   \n",
       "volume_imbalance_THREE_DAYS        425222.0 -0.016489  0.059414 -9.999861e-01   \n",
       "volume_imbalance_MINUTE            387306.0  0.020172  0.710638 -1.000000e+00   \n",
       "mle_alpha_powerlaw_FIVE_MINUTES    420374.0       inf       NaN -9.007199e+15   \n",
       "slippage_imbalance_THREE_DAYS      425211.0 -0.074882  0.248007 -1.000000e+00   \n",
       "log_return_HOUR                    422495.0 -0.000010  0.008247 -3.879387e-01   \n",
       "log_return_FOUR_HOURS              422641.0  0.000020  0.016864 -1.091786e+00   \n",
       "share_of_long_trades_FIVE_MINUTES  420374.0  0.512671  0.193722  0.000000e+00   \n",
       "slippage_imbalance_DAY             423377.0 -0.086343  0.323507 -1.000000e+00   \n",
       "slippage_imbalance_FOUR_HOURS      422333.0 -0.084263  0.481527 -1.000000e+00   \n",
       "mle_alpha_powerlaw_FOUR_HOURS      422641.0       inf       NaN  1.053494e+00   \n",
       "slippage_imbalance_MINUTE          260284.0  0.019955  0.887137 -1.000000e+00   \n",
       "volume_imbalance_HOUR              422495.0 -0.016561  0.229541 -1.000000e+00   \n",
       "slippage_imbalance_HALF_HOUR       419324.0 -0.047319  0.685021 -1.000000e+00   \n",
       "share_of_long_trades_FOUR_HOURS    422641.0  0.511803  0.080580  0.000000e+00   \n",
       "log_return_HALF_HOUR               422394.0 -0.000009  0.005747 -2.912688e-01   \n",
       "share_of_long_trades_MINUTE        387306.0  0.517020  0.300607  0.000000e+00   \n",
       "volume_imbalance_TWELVE_HOURS      422950.0 -0.018963  0.099134 -1.000000e+00   \n",
       "mle_alpha_powerlaw_THREE_DAYS      425222.0       inf       NaN  1.053684e+00   \n",
       "\n",
       "                                        25%       50%       75%       max  \n",
       "log_return_TWO_HOURS              -0.003859  0.000000  0.003891  0.767169  \n",
       "log_return_FIFTEEN_MINUTES        -0.001260  0.000000  0.001259  0.188052  \n",
       "volume_imbalance_THREE_DAYS       -0.040262 -0.015521  0.007661  1.000000  \n",
       "volume_imbalance_MINUTE           -0.654144  0.030000  0.705307  1.000000  \n",
       "mle_alpha_powerlaw_FIVE_MINUTES    1.320414  1.417477  1.580553       inf  \n",
       "slippage_imbalance_THREE_DAYS     -0.215651 -0.070563  0.063299  1.000000  \n",
       "log_return_HOUR                   -0.002684  0.000000  0.002661  0.340460  \n",
       "log_return_FOUR_HOURS             -0.005738  0.000000  0.005780  0.767169  \n",
       "share_of_long_trades_FIVE_MINUTES  0.400000  0.509969  0.631799  1.000000  \n",
       "slippage_imbalance_DAY            -0.290784 -0.083620  0.108531  1.000000  \n",
       "slippage_imbalance_FOUR_HOURS     -0.446715 -0.096129  0.251143  1.000000  \n",
       "mle_alpha_powerlaw_FOUR_HOURS      1.142608  1.189331  1.270284       inf  \n",
       "slippage_imbalance_MINUTE         -1.000000  0.063969  1.000000  1.000000  \n",
       "volume_imbalance_HOUR             -0.149253 -0.015223  0.115563  1.000000  \n",
       "slippage_imbalance_HALF_HOUR      -0.696307 -0.080916  0.582710  1.000000  \n",
       "share_of_long_trades_FOUR_HOURS    0.470588  0.511350  0.553527  1.000000  \n",
       "log_return_HALF_HOUR              -0.001787  0.000000  0.001784  0.215458  \n",
       "share_of_long_trades_MINUTE        0.333333  0.500000  0.730769  1.000000  \n",
       "volume_imbalance_TWELVE_HOURS     -0.066921 -0.016645  0.030042  1.000000  \n",
       "mle_alpha_powerlaw_THREE_DAYS      1.117648  1.136728  1.163328       inf  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[reg_cols].describe().T.iloc[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97272d97-7670-4afc-87e4-2bac3ae524cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix infinity alpha_powerlaw features. Clip to quantiles\n",
    "df[powerlaw_cols] = df[powerlaw_cols].replace(np.inf, np.nan)\n",
    "\n",
    "for col in powerlaw_cols:\n",
    "    df[col] = df[col].clip(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdc5d2-ab07-40f9-8e6d-bd39e2931b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[powerlaw_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e78f9-48f4-4af5-bd32-f5985ab7d785",
   "metadata": {},
   "source": [
    "<h4>Data preprocessing</h4>\n",
    "\n",
    "<p>Fill in nans in log_return feature with zeros, if there were no trades, hence the return is 0</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50660aa1-07c5-4378-804d-eefed1ac2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[return_cols] = df[return_cols].fillna(0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1216c55-1c9c-4d56-aa6c-745ec0f37f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df[return_cols] == 0).all(axis=1)].reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4f054-678d-4f96-92b8-dae0d201a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"return\"].between(-1, 1)].reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70441511-42a7-4309-911f-7be011ef4277",
   "metadata": {},
   "source": [
    "<p>Apply cross-sectional normalization and add cross-section id</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad281024-206a-4761-b91a-6d305b6756aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for i, ((_, _), df_cross_section) in enumerate(df.groupby([\"cross_section_start_time\", \"cross_section_end_time\"])):\n",
    "    df_cross_section[\"cross_section_id\"] = i\n",
    "    dfs.append(df_cross_section)\n",
    "\n",
    "df: pd.DataFrame = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b0a82-c706-4324-8c82-0bae68987fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of the data\n",
    "from tqdm import tqdm\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "df_plot = df[df[\"cross_section_id\"].isin([0, 200, 300])].copy()\n",
    "\n",
    "for ax, col in tqdm(zip(axs, return_cols)):\n",
    "    sns.histplot(\n",
    "        data=df_plot, x=col, hue=\"cross_section_id\", ax=ax, legend=False, alpha=0.05, bins=50, kde=True,\n",
    "        stat=\"probability\"\n",
    "    )\n",
    "    # ax.set_xlim([-0.005, 0.005])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d6fee-6404-4647-8b52-5922c43ca68f",
   "metadata": {},
   "source": [
    "<h4>Apply cross sectional standardization</h4>\n",
    "\n",
    "$$X_{\\text{standardized}} = \\frac{X - \\bar{X}_{\\text{within}}}{\\bar{\\sigma}(X)_{\\text{within}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8338426-e8ad-4a0d-b018-63a623c6e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for cross_section_id, df_cross_section in tqdm(df.groupby(\"cross_section_id\")):\n",
    "    for col in reg_cols:\n",
    "        df_cross_section[col] = (df_cross_section[col] - df_cross_section[col].mean()) / df_cross_section[col].std()\n",
    "    dfs.append(df_cross_section)\n",
    "\n",
    "df_scaled: pd.DataFrame = pd.concat(dfs)\n",
    "df_scaled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c0543-318e-425d-9252-3ebdec9da02f",
   "metadata": {},
   "source": [
    "<p>First we will setup this problem as the regression type problem</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e474b-6836-4d89-af40-78c003b4e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "df_plot_scaled = df_scaled[df_scaled[\"cross_section_id\"].isin([0, 100, 200])].copy()\n",
    "\n",
    "for ax, col in tqdm(zip(axs, return_cols)):\n",
    "    sns.histplot(\n",
    "        data=df_plot_scaled,\n",
    "        x=col, hue=\"cross_section_id\",\n",
    "        ax=ax, legend=False, alpha=0.05,\n",
    "        bins=50, kde=True, stat=\"probability\"\n",
    "    )\n",
    "    ax.set_xlim([-5, 5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8588c3-909c-4958-af5e-609f079471d1",
   "metadata": {},
   "source": [
    "<p><b>Result: </b>After applying the cross-sectional standardization, we see that the distributions are more aligned, this allows to fit models regardless of the market conditions as we are looking at features cross-sectionally. This will allow to do splits in trees models more robustly</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea837a91-6a0c-409d-9268-e2a3b9e43374",
   "metadata": {},
   "source": [
    "<h4>Remove obvious outliers in the target</h4>\n",
    "\n",
    "<p>We will remove all listings from our sample by removing first observation for each currency</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796695b-6659-48b0-9ea5-b82ae7fa69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = (\n",
    "    df_scaled\n",
    "    .sort_values(by=\"cross_section_start_time\", ascending=True)\n",
    "    .groupby(\"currency_pair\")\n",
    "    .nth(slice(1, None))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdedcd2-84e7-4816-bf1a-2edf5450172b",
   "metadata": {},
   "source": [
    "<h3>Modelling</h3>\n",
    "\n",
    "\n",
    "<h4>Baseline. Regression. Random Forest model</h4>\n",
    "\n",
    "<p>First we will do RandomForest model, we will do simple train, validation, test in chronological order to make sure that we don't have target leaking due to panel nature of our data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc57a2-23b8-40a2-b07d-bd9c34c67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional targets that we will use later before the splits\n",
    "df_scaled[\"asset_rank\"] = df_scaled.groupby(\"cross_section_id\")[\"return\"].rank(ascending=False)  # ranking target\n",
    "df_scaled[\"is_top_5\"] = df_scaled[\"asset_rank\"] <= 5  # classification target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080051a-0811-44fc-a397-d5486af3287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled[\"cross_section_end_time\"].agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcad2b4-b1ac-4f10-8349-f033198a87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled[\"return\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d348e-692a-40d9-8e44-3ad4cb6d3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "t0: datetime = datetime(2024, 2, 25)\n",
    "t1: datetime = datetime(2024, 3, 1)\n",
    "\n",
    "df_train, df_val, df_test = (\n",
    "    df_scaled[df_scaled[\"cross_section_end_time\"] < t0].copy(),\n",
    "    df_scaled[df_scaled[\"cross_section_end_time\"].between(t0, t1)].copy(),\n",
    "    df_scaled[df_scaled[\"cross_section_end_time\"] > t1].copy()\n",
    ")\n",
    "\n",
    "(\n",
    "    df_train[\"cross_section_id\"].nunique(),\n",
    "    df_val[\"cross_section_id\"].nunique(),\n",
    "    df_test[\"cross_section_id\"].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5082d-4c3c-4eab-a7b4-07e6d672d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureSet:\n",
    "    regressors: List[str]\n",
    "    target: str\n",
    "    categorical: Optional[List[str]] = None\n",
    "\n",
    "\n",
    "# initialize our feature set, that will be used throughtout the notebook\n",
    "reg_features: FeatureSet = FeatureSet(regressors=reg_cols, target=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35185f9c-a615-4cfa-a7bf-1394b27ef5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_base: RandomForestRegressor = RandomForestRegressor(\n",
    "    max_depth=5,\n",
    "    max_features=\"sqrt\",\n",
    "    n_estimators=100,\n",
    "    criterion=\"squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_rf_base.fit(X=df_train[reg_features.regressors], y=df_train[reg_features.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b7d43-7915-42d8-aacc-6889f6dcc918",
   "metadata": {},
   "source": [
    "<h4>Visualize results for baseline RandomForest model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd73b98-a241-411a-81d2-aa883b859c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred: np.ndarray = model_rf_base.predict(df_test[reg_features.regressors])\n",
    "\n",
    "plt.scatter(df_test[target_col], y_pred, label=\"Predicted by RF log_return\", alpha=.1)\n",
    "x_min, x_max = df_test[target_col].min(), df_val[target_col].max()\n",
    "\n",
    "X = np.linspace(x_min, x_max, 1000)\n",
    "plt.plot(X, X, linestyle=\"--\", color=\"red\")\n",
    "\n",
    "plt.title(\"Regression: Real log returns vs predictions by RF\")\n",
    "plt.xlabel(\"log_return\")\n",
    "plt.ylabel(\"log_return\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"rf_predictions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9aa64d-b00b-4881-907e-f0168a2b219a",
   "metadata": {},
   "source": [
    "<h4>Introduce business eval metric</h4>\n",
    "<p>It is not easy to interpret metrics like MAE, RMSE. We might want to look at things like SMAPE, MAPE which are more easily interpretable or look at business metrics like PnL</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daddce9-3a5f-4c3e-afa3-fb249c6957ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement trading strategy that we will run on the validation and test samples using our trained models\n",
    "# With prediction of returns it is quite simple, we will invest in top-5 highest return assets and sell \n",
    "# them once they are out of 5 best in the next prediction\n",
    "\n",
    "df_scaled[\"cross_section_start_time\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f3ecf-e94a-4d83-a405-7c48d7aa78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_returns_rf(model: RandomForestRegressor, feature_set: FeatureSet, df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Function that returns predictions for RF regressor model\"\"\"\n",
    "    return model.predict(X=df[feature_set.regressors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edeae1c-a179-40e4-a277-2b56ce68d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_strategy(df: pd.DataFrame, predicted_returns: np.ndarray) -> np.ndarray:\n",
    "    dfc: pd.DataFrame = df.copy()\n",
    "    dfc[\"predicted_return\"] = predicted_returns\n",
    "\n",
    "    portfolio: set[str] = set([])\n",
    "    portfolio_returns: List[float] = []\n",
    "\n",
    "    for cross_section_id, df_cross_section in dfc.groupby(\"cross_section_id\"):\n",
    "        # Get the list of currency_pairs with the best predicted performance\n",
    "        best_assets: set[str] = set(\n",
    "            df_cross_section.sort_values(\"predicted_return\", ascending=False)[\"currency_pair\"].iloc[:10].tolist()\n",
    "        )\n",
    "        buy_assets: set[str] = best_assets - portfolio  # assets that we end up buying\n",
    "        sell_assets: set[str] = portfolio - best_assets  # assets that we are selling when rebalancing\n",
    "        rebalancing_cost: float = (len(buy_assets) + len(sell_assets)) * 0.1 * 0.00075\n",
    "        portfolio_return: float = df_cross_section[df_cross_section[\"currency_pair\"].isin(best_assets)][\"return\"].mean() - rebalancing_cost\n",
    "\n",
    "        portfolio = best_assets\n",
    "        portfolio_returns.append(portfolio_return)\n",
    "\n",
    "    return np.array(portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd11453-0064-4ef3-9976-5a187a14c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred: np.ndarray = model_rf_base.predict(df_test[reg_features.regressors])\n",
    "\n",
    "rf_returns: np.ndarray = simple_strategy(df=df_test, predicted_returns=rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d542956-a0f0-471a-9821-c7136ef86f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rf_returns.cumsum())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34fc13-29e4-4eb4-9fd2-07a571045f3b",
   "metadata": {},
   "source": [
    "<h4>Regression. CatboostRegressor</h4>\n",
    "\n",
    "<p>Now attempt to use more complex boosting model in our regression problem. We will not mess around with hyperparameter tuning now, we first want to see if there is any hope for a good result. We will be training CatBoostRegressor with early stopping on a separate validation set with use_best_model flag set yo True which will remove latest <i>early_stopping_rounds</i> trees to the best model specification with highest validation score</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d8e12-877f-4f36-a895-5694c929a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "ptrain: Pool = Pool(data=df_train[reg_cols], label=df_train[target_col], cat_features=[\"currency_pair\"])\n",
    "pval: Pool = Pool(data=df_val[reg_cols], label=df_val[target_col], cat_features=[\"currency_pair\"])\n",
    "ptest: Pool = Pool(data=df_test[reg_cols], label=df_test[target_col], cat_features=[\"currency_pair\"])\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    objective=\"RMSE\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    verbose=False,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "_ = model.fit(\n",
    "    ptrain,\n",
    "    eval_set=pval,\n",
    "    plot=True,\n",
    "    early_stopping_rounds=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ae4d9-4fa4-445c-9268-6cf35afbf5c2",
   "metadata": {},
   "source": [
    "<h4>Find optimal hyperparameters that optimize RMSE on the validation set</h4>\n",
    "\n",
    "Since our $f(X, \\Theta)$ is not differentiable with respect to hyperparameters and are determined prior to loss optimisation, therefore we need to use Bayesian optimization to minimize our blackbox function on validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96506386-807e-48fc-96a3-eaca1933e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_REG_BASE_PARAMS: Dict[str, Any] = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"verbose\": False,\n",
    "    \"objective\": \"RMSE\",\n",
    "    \"use_best_model\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5f79e-e080-4b2e-8d75-e13acba8fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from optuna.study import StudyDirection\n",
    "from optuna.trial import Trial\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna import Study\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def catboost_regressor_objective(\n",
    "        trial: Trial, ptrain: Pool, pval: Pool, base_params: Dict[str, Any]\n",
    ") -> float:\n",
    "    suggested_params: Dict[str, Any] = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.7, 1),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "    }\n",
    "    # Add to base parameters, params suggested by optuna.Trial that minimizes objective -> float:\n",
    "    model_params: Dict[str, Any] = base_params | suggested_params\n",
    "\n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    model.fit(ptrain, eval_set=pval, early_stopping_rounds=50)\n",
    "\n",
    "    y_pred: np.ndarray = model.predict(pval)\n",
    "    rmse: float = root_mean_squared_error(y_true=pval.get_label(), y_pred=y_pred)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# Start the search for optimal hyperparams\n",
    "cb_reg_study: Study = optuna.create_study(\n",
    "    direction=StudyDirection.MINIMIZE,\n",
    "    pruner=HyperbandPruner(),\n",
    "    sampler=TPESampler(),\n",
    ")\n",
    "\n",
    "cb_reg_study.optimize(\n",
    "    partial(catboost_regressor_objective, ptrain=ptrain, pval=pval, base_params=CB_REG_BASE_PARAMS),\n",
    "    n_trials=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e62f7-3bc9-4c3d-8738-97a8e4b31322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params: Dict[str, Any] = CB_REG_BASE_PARAMS | cb_reg_study.best_params\n",
    "\n",
    "model_cb_tuned = CatBoostRegressor(**model_params)\n",
    "model_cb_tuned.fit(ptrain, eval_set=pval, early_stopping_rounds=50, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba38e0b-74dc-4f01-bb77-66eaf0878f2a",
   "metadata": {},
   "source": [
    "<p>Perhaps we can also adjust min_delta step because nothing much changes with validation RMSE, it still decreases but tiny bit</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b463bb-b2f2-48ff-8a68-1903e17515ba",
   "metadata": {},
   "source": [
    "<h4>Visualize results for CatboostRegressor</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11944d-8e35-4bdf-b90d-5c46e1c7cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred: np.ndarray = model_cb_tuned.predict(ptest)\n",
    "\n",
    "X = np.linspace(df_test[target_col].min(), df_test[target_col].max(), 1000)\n",
    "plt.plot(X, X, color=\"red\", linestyle=\"--\", label=\"Real Log return\")\n",
    "plt.scatter(df_test[target_col], y_pred, label=\"Predicted by CBRegressor log_return\")\n",
    "\n",
    "plt.title(\"Regression: Real log returns vs predictions by CBRegressor\")\n",
    "plt.xlabel(\"log_return\")\n",
    "plt.ylabel(\"log_return\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cb_predictons.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35ad4d-0e6e-4131-ad68-61cb01860478",
   "metadata": {},
   "source": [
    "<h4>Study feature importances for boosring model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab86e8-da9d-494e-94be-68083a03493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi: pd.DataFrame = pd.DataFrame({\n",
    "    \"feature\": model_cb_tuned.feature_names_,\n",
    "    \"feature_importance\": model_cb_tuned.feature_importances_\n",
    "}).sort_values(by=\"feature_importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(\n",
    "    data=df_fi.head(15),\n",
    "    x=\"feature_importance\",\n",
    "    y=\"feature\",\n",
    "    orient=\"h\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cb_feature_importances.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c3983-8af8-406f-a32f-0900fe6cb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_pred: np.ndarray = model_cb_tuned.predict(ptest)\n",
    "cb_returns: np.ndarray = simple_strategy(df=df_test, predicted_returns=cb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9457c-4e9a-472f-8ace-4bd73f23445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Regression models using our simple_strategy\n",
    "returns: List[np.ndarray] = (rf_returns.cumsum(), cb_returns.cumsum())\n",
    "models: List[str] = [\"RandomForestRegressor\", \"Tuned CatBoostRegressor\"]\n",
    "\n",
    "for return_series, model_name in zip(returns, models):\n",
    "    plt.plot(return_series, label=model_name)\n",
    "\n",
    "plt.title(\"Comparsion of models in terms of PnL\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pnl_validation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0eecfa-99f6-44f9-9a0f-d55a8c299e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_pred=cb_pred, y_true=df_test[\"log_return\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6de54a-bf16-40eb-b12e-4ded65b549ea",
   "metadata": {},
   "source": [
    "<h3>Ranking problem</h3>\n",
    "\n",
    "<p>We might also be willing to be able to predict not the returns themselves but ranking of assets based on their returns.</p>\n",
    "\n",
    "<h4>CatboostRanker</h4>\n",
    "\n",
    "<p>We see that high asset_rank corresponds to highest returns within each cross-section. We want a model that can correctly rank assets within each cross-section based on their returns from highest to lowest. So now we are not interested in getting precise values for returns we just want to be able to rank them properly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75103c-f8ef-4b59-95d1-2cc46834d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"asset_rank\"] == 1][[\"asset_rank\", \"log_return\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0db52-23ad-480d-a613-ea94e7e22a11",
   "metadata": {},
   "source": [
    "<p>To achieve this we will use CatBoostRanker implementation of Ranking models, this is the same boosting technique of estimation of f(x) but now our loss function is not RMSE or MAE that we might have used for regression but NDCG score which measures how correct the set is ranked given true rankings. In our case we will be using YetiRank loss which is just differentiable approximation to NDCG score that can be used in CatboostRanker as the target</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbccdc-2e89-45a1-9e5c-1174fa7b1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRanker\n",
    "\n",
    "ptrain: Pool = Pool(\n",
    "    data=df_train[reg_cols],\n",
    "    label=df_train[\"asset_rank\"],\n",
    "    cat_features=cat_cols,\n",
    "    group_id=df_train[\"cross_section_id\"]  # define cross_sections \n",
    ")\n",
    "\n",
    "pval: Pool = Pool(\n",
    "    data=df_val[reg_cols],\n",
    "    label=df_val[\"asset_rank\"],\n",
    "    cat_features=cat_cols,\n",
    "    group_id=df_val[\"cross_section_id\"]\n",
    ")\n",
    "\n",
    "model_ranker = CatBoostRanker(\n",
    "    objective=\"YetiRankPairwise:mode=NDCG\",\n",
    "    verbose=False,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "model_ranker.fit(\n",
    "    ptrain, eval_set=pval, early_stopping_rounds=5, plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5119e0-10dd-4227-af35-325e10132c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_scores: np.ndarray = model.predict(ptest)\n",
    "\n",
    "df_test[\"relevance_scores\"] = relevance_scores\n",
    "df_test[\"predicted_rank\"] = df_test.groupby(\"cross_section_id\")[\"relevance_scores\"].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7bb25a-bc8b-4f71-a7f5-88e97725e5f0",
   "metadata": {},
   "source": [
    "<h4>Compute PnL using Ranker model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595e476-7564-4bcb-8db1-4393af0d56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns: List[float] = []\n",
    "\n",
    "portfolio_returns = []\n",
    "portfolio = set([])\n",
    "\n",
    "for cross_section_id, df_cross_section in df_test.groupby(\"cross_section_id\"):\n",
    "    best_assets: set[str] = set(\n",
    "        df_cross_section[df_cross_section[\"predicted_rank\"] <= 10][\"currency_pair\"].tolist()\n",
    "    )\n",
    "    buy_assets: set[str] = best_assets - portfolio  # assets that we end up buying\n",
    "    sell_assets: set[str] = portfolio - best_assets  # assets that we are selling when rebalancing\n",
    "    rebalancing_cost: float = (len(buy_assets) + len(sell_assets)) * 0.1 * 0.00075\n",
    "    portfolio_return: float = df_cross_section[df_cross_section[\"currency_pair\"].isin(best_assets)][\"log_return\"].mean() - rebalancing_cost\n",
    "\n",
    "    portfolio = best_assets\n",
    "    portfolio_returns.append(portfolio_return)\n",
    "\n",
    "ranker_returns: np.ndarray = np.array(portfolio_returns)\n",
    "plt.plot(ranker_returns.cumsum())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754a419-1397-41df-bb91-96ae8f70fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns: List[np.ndarray] = [\n",
    "    rf_returns.cumsum(),\n",
    "    cb_returns.cumsum(),\n",
    "    ranker_returns.cumsum()\n",
    "]\n",
    "models: List[str] = [\"RandomForestRegressor\", \"Tuned CatBoostRegressor\", \"CatBoostRanker\"]\n",
    "\n",
    "for return_series, model_name in zip(returns, models):\n",
    "    plt.plot(return_series, label=model_name)\n",
    "\n",
    "# plt.plot((1 + df_test[df_test[\"currency_pair\"] == \"BTCUSDT\"][\"log_return\"]).cumprod().values)\n",
    "\n",
    "plt.title(\"Comparsion of models in terms of PnL\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pnl_validation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc4c61-6822-4c89-97e0-d1e05de1fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns: List[np.ndarray] = [\n",
    "    rf_returns.cumsum(),\n",
    "    cb_returns.cumsum(),\n",
    "    ranker_returns.cumsum(),\n",
    "]\n",
    "\n",
    "models: List[str] = [\"RandomForestRegressor\", \"Tuned CatBoostRegressor\", \"CatBoostRanker\"]\n",
    "X = df_test[\"cross_section_start_time\"].unique()\n",
    "\n",
    "for return_series, model_name in zip(returns, models):\n",
    "    plt.plot(X[:270], return_series[:270], label=model_name)\n",
    "\n",
    "plt.plot(\n",
    "    X[:270],\n",
    "    df_test[df_test[\"currency_pair\"] == \"BTCUSDT\"][\"log_return\"].cumsum().values[:270],\n",
    "    label=\"BTC hold\"\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.title(\"Comparsion of models in terms of PnL\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pnl_validation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987b039-8b39-4abe-a024-fe157dd071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker_returns.mean() / ranker_returns.std() * np.sqrt(365)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
